{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11586516,"sourceType":"datasetVersion","datasetId":7263310}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Removing spin_id,3d_model_id,node_id\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\nfrom collections import Counter\nimport os\nimport json\nfrom glob import glob\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:56:36.984876Z","iopub.execute_input":"2025-05-18T07:56:36.985085Z","iopub.status.idle":"2025-05-18T07:56:37.351235Z","shell.execute_reply.started":"2025-05-18T07:56:36.985064Z","shell.execute_reply":"2025-05-18T07:56:37.350448Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom glob import glob\nimport kagglehub\n\ndataset_dir = kagglehub.dataset_download(\"shaikmdirfan/images\")\nmetadata_dir = os.path.join(dataset_dir, 'metadata/metadata')\n\noutput_dir = \"/kaggle/working/cleaned_metadata\"\nos.makedirs(output_dir, exist_ok=True)\n\njson_files = glob(os.path.join(metadata_dir, \"listings_*.json\"))\n\nfor file_path in json_files:\n    cleaned_data = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue \n\n            try:\n                entry = json.loads(line)\n\n                for unwanted in [\"spin_id\", \"3dmodel_id\"]:\n                    if unwanted in entry:\n                        del entry[unwanted]\n\n                if isinstance(entry.get(\"node\"), list):\n                    node_list = entry[\"node\"]\n                    if len(node_list) == 1 and isinstance(node_list[0], dict):\n                        node_item = node_list[0]\n                        if \"node_name\" in node_item:\n                            entry[\"node_name\"] = node_item[\"node_name\"]\n                    # Remove the original \"node\" key\n                    entry.pop(\"node\", None)\n\n\n                cleaned_data.append(entry)\n\n            except json.JSONDecodeError as e:\n                print(f\" Skipping malformed JSON in {file_path}: {e}\")\n                continue\n\n    output_path = os.path.join(output_dir, os.path.basename(file_path))\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        for entry in cleaned_data:\n            json.dump(entry, f, ensure_ascii=False)\n            f.write(\"\\n\")\n\nprint(f\"✅ Cleaned {len(json_files)} files. Output saved to: {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:56:37.352958Z","iopub.execute_input":"2025-05-18T07:56:37.353172Z","iopub.status.idle":"2025-05-18T07:57:45.014447Z","shell.execute_reply.started":"2025-05-18T07:56:37.353150Z","shell.execute_reply":"2025-05-18T07:57:45.013645Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned 16 files. Output saved to: /kaggle/working/cleaned_metadata\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"json_files = glob(os.path.join(metadata_dir, \"listings_*.json\"))\nfor file_path in json_files:\n    count = 0\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip(): \n                count += 1\n    file_name = os.path.basename(file_path)\n    print(f\"{file_name}: {count} entries\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:57:45.015101Z","iopub.execute_input":"2025-05-18T07:57:45.015412Z","iopub.status.idle":"2025-05-18T07:57:45.782303Z","shell.execute_reply.started":"2025-05-18T07:57:45.015386Z","shell.execute_reply":"2025-05-18T07:57:45.781601Z"}},"outputs":[{"name":"stdout","text":"listings_3.json: 9232 entries\nlistings_d.json: 9232 entries\nlistings_b.json: 9232 entries\nlistings_a.json: 9232 entries\nlistings_4.json: 9232 entries\nlistings_8.json: 9232 entries\nlistings_9.json: 9232 entries\nlistings_f.json: 9222 entries\nlistings_0.json: 9232 entries\nlistings_6.json: 9232 entries\nlistings_c.json: 9232 entries\nlistings_2.json: 9232 entries\nlistings_5.json: 9232 entries\nlistings_1.json: 9232 entries\nlistings_e.json: 9232 entries\nlistings_7.json: 9232 entries\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"cleaned_dir = \"/kaggle/working/cleaned_metadata\"\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nerrors_found = False\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line_num, line in enumerate(f, start=1):\n            try:\n                entry = json.loads(line)\n\n                if 'spin_id' in entry:\n                    print(f\"'spin_id' found in {file_path}, line {line_num}\")\n                    errors_found = True\n\n                if '3dmodel_id' in entry:\n                    print(f\"'3dmodel_id' found in {file_path}, line {line_num}\")\n                    errors_found = True\n\n                \n\n            except json.JSONDecodeError as e:\n                print(f\"JSON error in {file_path}, line {line_num}: {e}\")\n                errors_found = True\n\nif not errors_found:\n    print(\"All cleaned files are verified — no 'spin_id', '3dmodel_id', or 'node_id' found.\")\nelse:\n    print(\"Some unwanted keys still exist. See messages above.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:57:45.783103Z","iopub.execute_input":"2025-05-18T07:57:45.783468Z","iopub.status.idle":"2025-05-18T07:57:54.229242Z","shell.execute_reply.started":"2025-05-18T07:57:45.783444Z","shell.execute_reply":"2025-05-18T07:57:54.228636Z"}},"outputs":[{"name":"stdout","text":"All cleaned files are verified — no 'spin_id', '3dmodel_id', or 'node_id' found.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Removing data of language tags other than En_**","metadata":{}},{"cell_type":"code","source":"\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\n\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nlanguage_tags = set()\nfields_with_language_tag = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                for field in fields_with_language_tag:\n                    if field in entry and isinstance(entry[field], list):\n                        for item in entry[field]:\n                            lang = item.get(\"language_tag\")\n                            if lang:\n                                language_tags.add(lang)\n            except json.JSONDecodeError as e:\n                print(f\"Skipping line due to JSON error in {file_path}: {e}\")\n\nprint(\"Unique language_tag values:\")\nfor lang in sorted(language_tags):\n    print(lang)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:57:54.229885Z","iopub.execute_input":"2025-05-18T07:57:54.230160Z","iopub.status.idle":"2025-05-18T07:58:05.387072Z","shell.execute_reply.started":"2025-05-18T07:57:54.230112Z","shell.execute_reply":"2025-05-18T07:58:05.386371Z"}},"outputs":[{"name":"stdout","text":"Unique language_tag values:\nar_AE\ncs_CZ\nde_DE\nen_AE\nen_AU\nen_CA\nen_GB\nen_IN\nen_SG\nen_US\nes_ES\nes_MX\nes_US\nfr_CA\nfr_FR\nhe_IL\nhi_IN\nit_IT\nja_JP\nkn_IN\nko_KR\nml_IN\nmr_IN\nnl_NL\npl_PL\npt_BR\npt_PT\nsv_SE\nta_IN\nte_IN\ntr_TR\nzh_CN\nzh_TW\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(f\"\\nNumber of unique language_tag values: {len(language_tags)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:05.387980Z","iopub.execute_input":"2025-05-18T07:58:05.388222Z","iopub.status.idle":"2025-05-18T07:58:05.392394Z","shell.execute_reply.started":"2025-05-18T07:58:05.388197Z","shell.execute_reply":"2025-05-18T07:58:05.391693Z"}},"outputs":[{"name":"stdout","text":"\nNumber of unique language_tag values: 33\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport json\nfrom glob import glob\n\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\n\nfields_with_language_tag = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\ndef is_english_or_missing(tag):\n    # Accept if it's English or missing\n    return tag is None or (isinstance(tag, str) and tag.lower().startswith(\"en_\"))\n\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfor file_path in json_files:\n    filtered_entries = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                # Filter each specified field\n                for field in fields_with_language_tag:\n                    if field in entry and isinstance(entry[field], list):\n                        filtered_field_items = []\n                        for item in entry[field]:\n                            tag = item.get(\"language_tag\", None)\n                            if is_english_or_missing(tag):\n                                filtered_field_items.append(item)\n                        entry[field] = filtered_field_items\n\n                filtered_entries.append(entry)\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\n    # Safely overwrite the same file\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for entry in filtered_entries:\n            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n\nprint(\"Files updated: retained items without 'language_tag' and removed non-English ones.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:05.394526Z","iopub.execute_input":"2025-05-18T07:58:05.394708Z","iopub.status.idle":"2025-05-18T07:58:40.161099Z","shell.execute_reply.started":"2025-05-18T07:58:05.394694Z","shell.execute_reply":"2025-05-18T07:58:40.160342Z"}},"outputs":[{"name":"stdout","text":"Files updated: retained items without 'language_tag' and removed non-English ones.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\n\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nlanguage_tags = set()\nfields_with_language_tag = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                for field in fields_with_language_tag:\n                    if field in entry and isinstance(entry[field], list):\n                        for item in entry[field]:\n                            lang = item.get(\"language_tag\")\n                            if lang:\n                                language_tags.add(lang)\n            except json.JSONDecodeError as e:\n                print(f\"Skipping line due to JSON error in {file_path}: {e}\")\n\nprint(\"Unique language_tag values:\")\nfor lang in sorted(language_tags):\n    print(lang)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:40.162043Z","iopub.execute_input":"2025-05-18T07:58:40.162504Z","iopub.status.idle":"2025-05-18T07:58:49.369901Z","shell.execute_reply.started":"2025-05-18T07:58:40.162467Z","shell.execute_reply":"2025-05-18T07:58:49.369168Z"}},"outputs":[{"name":"stdout","text":"Unique language_tag values:\nen_AE\nen_AU\nen_CA\nen_GB\nen_IN\nen_SG\nen_US\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import json\nfrom glob import glob\nimport os\n\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfields_to_count = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\nfield_counts = {field: 0 for field in fields_to_count}\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                # Count occurrences of each field\n                for field in fields_to_count:\n                    if field in entry:\n                        field_counts[field] += 1\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\n# Print the counts for each field\nprint(\"\\nField Occurrence Counts:\")\nfor field, count in field_counts.items():\n    print(f\"{field}: {count}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:49.370709Z","iopub.execute_input":"2025-05-18T07:58:49.371358Z","iopub.status.idle":"2025-05-18T07:58:56.463330Z","shell.execute_reply.started":"2025-05-18T07:58:49.371331Z","shell.execute_reply":"2025-05-18T07:58:56.462677Z"}},"outputs":[{"name":"stdout","text":"\nField Occurrence Counts:\nbrand: 147643\nbullet_point: 131570\nfabric_type: 8193\nfinish_type: 1536\nitem_keywords: 126776\nitem_name: 147702\nitem_shape: 5066\nmaterial: 53585\nmodel_name: 81579\nmodel_number: 124091\nmodel_year: 7765\npattern: 4590\nproduct_description: 4240\nstyle: 43188\ncolor: 116180\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"json_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\n# Fields to process\nfields_with_language_tag = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\nfor file_path in json_files:\n    updated_entries = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                for field in fields_with_language_tag:\n                    if field in entry and isinstance(entry[field], list):\n                        # Extract 'value' and ignore other keys\n                        entry[field] = [item[\"value\"] for item in entry[field] if \"value\" in item]\n\n                updated_entries.append(entry)\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for entry in updated_entries:\n            json.dump(entry, f, ensure_ascii=False)\n            f.write(\"\\n\")\n\nprint(\"Successfully removed 'language_tag' and converted field values to lists of strings.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:58:56.463985Z","iopub.execute_input":"2025-05-18T07:58:56.464180Z","iopub.status.idle":"2025-05-18T07:59:20.991776Z","shell.execute_reply.started":"2025-05-18T07:58:56.464166Z","shell.execute_reply":"2025-05-18T07:59:20.991167Z"}},"outputs":[{"name":"stdout","text":"Successfully removed 'language_tag' and converted field values to lists of strings.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"json_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfields_to_check = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\ninvalid_entries = 0\nvalid_entries = 0\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                for field in fields_to_check:\n                    if field in entry:\n                        if field == \"model_year\":\n                            if not isinstance(entry[field], list) or not all(isinstance(x, int) for x in entry[field]):\n                                invalid_entries += 1\n                                print(f\"Invalid format in field '{field}' for item_id: {entry.get('item_id')}\")\n                            else:\n                                valid_entries += 1\n                        else:\n                            if not isinstance(entry[field], list) or not all(isinstance(x, str) for x in entry[field]):\n                                invalid_entries += 1\n                                print(f\"Invalid format in field '{field}' for item_id: {entry.get('item_id')}\")\n                            else:\n                                valid_entries += 1\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(\"\\nVerification Complete\")\nprint(f\"Valid string list fields: {valid_entries}\")\nprint(f\"Invalid entries (non-list or non-string values): {invalid_entries}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:20.992625Z","iopub.execute_input":"2025-05-18T07:59:20.992897Z","iopub.status.idle":"2025-05-18T07:59:24.965725Z","shell.execute_reply.started":"2025-05-18T07:59:20.992877Z","shell.execute_reply":"2025-05-18T07:59:24.965115Z"}},"outputs":[{"name":"stdout","text":"\nVerification Complete\nValid string list fields: 1003704\nInvalid entries (non-list or non-string values): 0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\nfrom glob import glob\nimport os\n\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfields_to_count = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\nfield_counts = {field: 0 for field in fields_to_count}\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                for field in fields_to_count:\n                    if field in entry:\n                        field_counts[field] += 1\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(\"\\nField Occurrence Counts:\")\nfor field, count in field_counts.items():\n    print(f\"{field}: {count}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:24.966405Z","iopub.execute_input":"2025-05-18T07:59:24.966620Z","iopub.status.idle":"2025-05-18T07:59:28.068753Z","shell.execute_reply.started":"2025-05-18T07:59:24.966603Z","shell.execute_reply":"2025-05-18T07:59:28.068112Z"}},"outputs":[{"name":"stdout","text":"\nField Occurrence Counts:\nbrand: 147643\nbullet_point: 131570\nfabric_type: 8193\nfinish_type: 1536\nitem_keywords: 126776\nitem_name: 147702\nitem_shape: 5066\nmaterial: 53585\nmodel_name: 81579\nmodel_number: 124091\nmodel_year: 7765\npattern: 4590\nproduct_description: 4240\nstyle: 43188\ncolor: 116180\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"* Field Occurrence Counts:\n* brand: 147643\n* bullet_point: 131570\n* fabric_type: 8193\n* finish_type: 1536\n* item_keywords: 126776\n* item_name: 147702\n* item_shape: 5066\n* material: 53585\n* model_name: 81579\n* model_number: 124091\n* model_year: 7765\n* pattern: 4590\n* product_description: 4240\n* style: 43188\n* color: 116180","metadata":{}},{"cell_type":"code","source":"d = 0\nfor file_path in json_files:\n    count = 0\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip():  # skip empty lines, just in case\n                count += 1\n        d+=count\n    file_name = os.path.basename(file_path)\n    print(f\"{file_name}: {count} entries\")\nprint(d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:28.069492Z","iopub.execute_input":"2025-05-18T07:59:28.069733Z","iopub.status.idle":"2025-05-18T07:59:28.495731Z","shell.execute_reply.started":"2025-05-18T07:59:28.069714Z","shell.execute_reply":"2025-05-18T07:59:28.495176Z"}},"outputs":[{"name":"stdout","text":"listings_0.json: 9232 entries\nlistings_5.json: 9232 entries\nlistings_d.json: 9232 entries\nlistings_2.json: 9232 entries\nlistings_a.json: 9232 entries\nlistings_3.json: 9232 entries\nlistings_6.json: 9232 entries\nlistings_1.json: 9232 entries\nlistings_b.json: 9232 entries\nlistings_f.json: 9222 entries\nlistings_8.json: 9232 entries\nlistings_7.json: 9232 entries\nlistings_c.json: 9232 entries\nlistings_9.json: 9232 entries\nlistings_4.json: 9232 entries\nlistings_e.json: 9232 entries\n147702\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Checking unique Domain names","metadata":{}},{"cell_type":"code","source":"domain_names = set()\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                domain_name = entry.get(\"domain_name\")\n                if domain_name:\n                    domain_names.add(domain_name)\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(\"Unique domain_name values:\")\nfor domain in sorted(domain_names):\n    print(domain)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:28.496540Z","iopub.execute_input":"2025-05-18T07:59:28.496754Z","iopub.status.idle":"2025-05-18T07:59:31.354763Z","shell.execute_reply.started":"2025-05-18T07:59:28.496738Z","shell.execute_reply":"2025-05-18T07:59:31.353966Z"}},"outputs":[{"name":"stdout","text":"Unique domain_name values:\namazon.ae\namazon.ca\namazon.co.jp\namazon.co.uk\namazon.com\namazon.com.au\namazon.com.br\namazon.com.mx\namazon.com.tr\namazon.com/go\namazon.de\namazon.es\namazon.fr\namazon.in\namazon.it\namazon.nl\namazon.pl\namazon.sa\namazon.se\namazon.sg\namazondistribution.in\nfresh.amazon.com\nprimenow.amazon.ca\nprimenow.amazon.co.jp\nprimenow.amazon.co.uk\nprimenow.amazon.com\nprimenow.amazon.de\nprimenow.amazon.es\nprimenow.amazon.fr\nprimenow.amazon.it\nwholefoodsmarket.com\nwoot.com\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Checking unique item_ids","metadata":{}},{"cell_type":"code","source":"item_ids = set()\nc=0\nd=0\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                c += 1\n                item_id = entry.get(\"item_id\")\n                if item_id:\n                    d+=1\n                    item_ids.add(item_id)\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(f\"Number of unique item_id values: {len(item_ids)}\")\nprint(f\"Total Entries:{c}\")\nprint(f\"Total Entries with item_id:{d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:31.355627Z","iopub.execute_input":"2025-05-18T07:59:31.355864Z","iopub.status.idle":"2025-05-18T07:59:34.213064Z","shell.execute_reply.started":"2025-05-18T07:59:31.355845Z","shell.execute_reply":"2025-05-18T07:59:34.212482Z"}},"outputs":[{"name":"stdout","text":"Number of unique item_id values: 145615\nTotal Entries:147702\nTotal Entries with item_id:147702\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Checking Normalised values and other values in item_dimensions and item_weight","metadata":{}},{"cell_type":"code","source":"dimensions_mismatch = 0\ndimensions_checked = 0\n\nweights_mismatch = 0\nweights_checked = 0\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                dims = entry.get(\"item_dimensions\", {})\n                if isinstance(dims, dict):\n                    for dim_key, dim_val in dims.items():\n                        if isinstance(dim_val, dict):\n                            norm = dim_val.get(\"normalized_value\", {})\n                            if all(k in dim_val for k in [\"unit\", \"value\"]) and all(k in norm for k in [\"unit\", \"value\"]):\n                                dimensions_checked += 1\n                                if dim_val[\"unit\"] != norm[\"unit\"] or dim_val[\"value\"] != norm[\"value\"]:\n                                    dimensions_mismatch += 1\n\n                weights = entry.get(\"item_weight\", [])\n                if isinstance(weights, list):\n                    for w in weights:\n                        norm = w.get(\"normalized_value\", {})\n                        if all(k in w for k in [\"unit\", \"value\"]) and all(k in norm for k in [\"unit\", \"value\"]):\n                            weights_checked += 1\n                            if w[\"unit\"] != norm[\"unit\"] or w[\"value\"] != norm[\"value\"]:\n                                weights_mismatch += 1\n\n            except json.JSONDecodeError:\n                continue\n\nprint(\"item_dimensions:\")\nprint(f\"  Total checked: {dimensions_checked}\")\nprint(f\"  Mismatches: {dimensions_mismatch}\")\n\nprint(\"item_weight:\")\nprint(f\"  Total checked: {weights_checked}\")\nprint(f\"  Mismatches: {weights_mismatch}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:34.213733Z","iopub.execute_input":"2025-05-18T07:59:34.213929Z","iopub.status.idle":"2025-05-18T07:59:37.501934Z","shell.execute_reply.started":"2025-05-18T07:59:34.213914Z","shell.execute_reply":"2025-05-18T07:59:37.501110Z"}},"outputs":[{"name":"stdout","text":"item_dimensions:\n  Total checked: 130014\n  Mismatches: 26040\nitem_weight:\n  Total checked: 106194\n  Mismatches: 78270\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n\nmissing_keys_report = {\n    \"item_dimensions\": [],\n    \"item_weight\": []\n}\n\nrequired_keys = [\"value\", \"unit\", \"normalized_value\"]\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line_number, line in enumerate(f, 1):\n            try:\n                entry = json.loads(line)\n                for field in [\"item_dimensions\", \"item_weight\"]:\n                    if field in entry and isinstance(entry[field], list):\n                        for idx, item in enumerate(entry[field]):\n                            for key in required_keys:\n                                if key not in item:\n                                    missing_keys_report[field].append({\n                                        \"file\": file_path,\n                                        \"line\": line_number,\n                                        \"missing_key\": key,\n                                        \"entry_index\": idx\n                                    })\n            except json.JSONDecodeError:\n                continue\n\nfor field, issues in missing_keys_report.items():\n    print(f\"\\n {len(issues)} missing '{field}' keys:\")\n    for issue in issues[:10]:  # Show only first 10 for brevity\n        print(issue)\n\nprint(\"\\nCheck complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:37.502813Z","iopub.execute_input":"2025-05-18T07:59:37.503501Z","iopub.status.idle":"2025-05-18T07:59:40.451632Z","shell.execute_reply.started":"2025-05-18T07:59:37.503472Z","shell.execute_reply":"2025-05-18T07:59:40.450920Z"}},"outputs":[{"name":"stdout","text":"\n 0 missing 'item_dimensions' keys:\n\n 0 missing 'item_weight' keys:\n\nCheck complete.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n\ntotal_files = 0\n\ntotal_weight_checked = 0\nweight_missing_value_or_unit = 0\nweight_found_normalized_value = 0\n\ntotal_dimensions_checked = 0\ndimensions_missing_value_or_unit = 0\ndimensions_found_normalized_value = 0\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                total_files += 1\n\n                if \"item_weight\" in entry and isinstance(entry[\"item_weight\"], list):\n                    for item in entry[\"item_weight\"]:\n                        total_weight_checked += 1\n                        if \"value\" not in item or \"unit\" not in item:\n                            weight_missing_value_or_unit += 1\n                        if \"normalized_value\" in item:\n                            weight_found_normalized_value += 1\n\n                if \"item_dimensions\" in entry and isinstance(entry[\"item_dimensions\"], dict):\n                    for dimension, dimension_details in entry[\"item_dimensions\"].items():\n                        total_dimensions_checked += 1\n                        if \"value\" not in dimension_details or \"unit\" not in dimension_details:\n                            dimensions_missing_value_or_unit += 1\n                        if \"normalized_value\" in dimension_details:\n                            dimensions_found_normalized_value += 1\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(\" Validation Summary:\")\nprint(f\"Total entries processed: {total_files}\\n\")\n\nprint(\" item_weight checks:\")\nprint(f\"  Total checked: {total_weight_checked}\")\nprint(f\"  Missing 'value' or 'unit': {weight_missing_value_or_unit}\")\nprint(f\"  Containing 'normalized_value': {weight_found_normalized_value}\\n\")\n\nprint(\" item_dimensions checks:\")\nprint(f\"  Total checked: {total_dimensions_checked}\")\nprint(f\"  Missing 'value' or 'unit': {dimensions_missing_value_or_unit}\")\nprint(f\"  Containing 'normalized_value': {dimensions_found_normalized_value}\")\n\nif all(val == 0 for val in [\n    weight_missing_value_or_unit,\n    weight_found_normalized_value,\n    dimensions_missing_value_or_unit,\n    dimensions_found_normalized_value\n]):\n    print(\"\\n All entries are valid: have 'value' and 'unit', and no 'normalized_value'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:40.452700Z","iopub.execute_input":"2025-05-18T07:59:40.452958Z","iopub.status.idle":"2025-05-18T07:59:43.452042Z","shell.execute_reply.started":"2025-05-18T07:59:40.452933Z","shell.execute_reply":"2025-05-18T07:59:43.451384Z"}},"outputs":[{"name":"stdout","text":" Validation Summary:\nTotal entries processed: 147702\n\n item_weight checks:\n  Total checked: 106194\n  Missing 'value' or 'unit': 0\n  Containing 'normalized_value': 106194\n\n item_dimensions checks:\n  Total checked: 130014\n  Missing 'value' or 'unit': 0\n  Containing 'normalized_value': 130014\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfor file_path in json_files:\n    updated_entries = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                if \"item_weight\" in entry and isinstance(entry[\"item_weight\"], list):\n                    for weight in entry[\"item_weight\"]:\n                        weight.pop(\"normalized_value\", None)\n\n                if \"item_dimensions\" in entry and isinstance(entry[\"item_dimensions\"], dict):\n                    for key in entry[\"item_dimensions\"]:\n                        if isinstance(entry[\"item_dimensions\"][key], dict):\n                            entry[\"item_dimensions\"][key].pop(\"normalized_value\", None)\n\n                updated_entries.append(entry)\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for entry in updated_entries:\n            json.dump(entry, f, ensure_ascii=False)\n            f.write(\"\\n\")\n\nprint(\" Removed 'normalized_value' from item_weight and item_dimensions in all files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T07:59:43.452805Z","iopub.execute_input":"2025-05-18T07:59:43.453008Z","iopub.status.idle":"2025-05-18T08:00:01.170097Z","shell.execute_reply.started":"2025-05-18T07:59:43.452994Z","shell.execute_reply":"2025-05-18T08:00:01.169367Z"}},"outputs":[{"name":"stdout","text":" Removed 'normalized_value' from item_weight and item_dimensions in all files.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"total_files = 0\n\ntotal_weight_checked = 0\nweight_has_value = 0\nweight_has_unit = 0\nweight_found_normalized_value = 0\n\ntotal_dimensions_checked = 0\ndimensions_has_value = 0\ndimensions_has_unit = 0\ndimensions_found_normalized_value = 0\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                total_files += 1\n\n                if \"item_weight\" in entry and isinstance(entry[\"item_weight\"], list):\n                    for item in entry[\"item_weight\"]:\n                        total_weight_checked += 1\n                        if \"value\" in item:\n                            weight_has_value += 1\n                        if \"unit\" in item:\n                            weight_has_unit += 1\n                        if \"normalized_value\" in item:\n                            weight_found_normalized_value += 1\n\n                if \"item_dimensions\" in entry and isinstance(entry[\"item_dimensions\"], dict):\n                    for dimension, dimension_details in entry[\"item_dimensions\"].items():\n                        total_dimensions_checked += 1\n                        if \"value\" in dimension_details:\n                            dimensions_has_value += 1\n                        if \"unit\" in dimension_details:\n                            dimensions_has_unit += 1\n                        if \"normalized_value\" in dimension_details:\n                            dimensions_found_normalized_value += 1\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\nprint(\" Validation Summary:\")\nprint(f\"Total entries processed: {total_files}\\n\")\n\nprint(\" item_weight checks:\")\nprint(f\"  Total checked: {total_weight_checked}\")\nprint(f\"  Entries with 'value': {weight_has_value}\")\nprint(f\"  Entries with 'unit': {weight_has_unit}\")\nprint(f\"  Containing 'normalized_value': {weight_found_normalized_value}\\n\")\n\nprint(\" item_dimensions checks:\")\nprint(f\"  Total checked: {total_dimensions_checked}\")\nprint(f\"  Entries with 'value': {dimensions_has_value}\")\nprint(f\"  Entries with 'unit': {dimensions_has_unit}\")\nprint(f\"  Containing 'normalized_value': {dimensions_found_normalized_value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:00:01.170951Z","iopub.execute_input":"2025-05-18T08:00:01.171305Z","iopub.status.idle":"2025-05-18T08:00:03.954873Z","shell.execute_reply.started":"2025-05-18T08:00:01.171278Z","shell.execute_reply":"2025-05-18T08:00:03.954289Z"}},"outputs":[{"name":"stdout","text":" Validation Summary:\nTotal entries processed: 147702\n\n item_weight checks:\n  Total checked: 106194\n  Entries with 'value': 106194\n  Entries with 'unit': 106194\n  Containing 'normalized_value': 0\n\n item_dimensions checks:\n  Total checked: 130014\n  Entries with 'value': 130014\n  Entries with 'unit': 130014\n  Containing 'normalized_value': 0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Checking mapping between images and json files","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom glob import glob\n\ncleaned_dir = \"/kaggle/working/cleaned_metadata\"\njson_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nmain_image_ids = set()\nother_image_ids = set()\nall_unique_ids = set()\n\nfor file_path in json_files:\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n\n                main_id = entry.get(\"main_image_id\")\n                if main_id:\n                    main_image_ids.add(main_id)\n                    all_unique_ids.add(main_id)\n\n                other_ids = entry.get(\"other_image_id\", [])\n                if isinstance(other_ids, list):\n                    for oid in other_ids:\n                        if oid:\n                            other_image_ids.add(oid)\n                            all_unique_ids.add(oid)\n\n            except json.JSONDecodeError:\n                continue\n\nprint(f\"Unique main_image_id count: {len(main_image_ids)}\")\nprint(f\"Unique other_image_ids count: {len(other_image_ids)}\")\nprint(f\"Unique ids count: {len(all_unique_ids)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:00:03.955534Z","iopub.execute_input":"2025-05-18T08:00:03.955771Z","iopub.status.idle":"2025-05-18T08:00:06.975878Z","shell.execute_reply.started":"2025-05-18T08:00:03.955751Z","shell.execute_reply":"2025-05-18T08:00:06.975337Z"}},"outputs":[{"name":"stdout","text":"Unique main_image_id count: 123511\nUnique other_image_ids count: 275505\nUnique ids count: 398170\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"json_files = glob(os.path.join(cleaned_dir, \"listings_*.json\"))\n\nfields_to_dedup = [\n    \"brand\", \"bullet_point\", \"fabric_type\", \"finish_type\", \"item_keywords\",\n    \"item_name\", \"item_shape\", \"material\", \"model_name\", \"model_number\",\n    \"model_year\", \"pattern\", \"product_description\", \"style\", \"color\"\n]\n\ndef deduplicate_list(lst):\n    return list(dict.fromkeys(lst))\n\nexamples_shown = 0\nMAX_EXAMPLES = 3\n\nfor file_path in json_files:\n    updated_lines = []\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                entry = json.loads(line)\n                modified = False\n                item_id = entry.get(\"item_id\", \"UNKNOWN_ID\")\n                \n                for field in fields_to_dedup:\n                    if field in entry and isinstance(entry[field], list):\n                        original = entry[field]\n                        deduped = deduplicate_list(original)\n                        if len(original) != len(deduped):\n                            entry[field] = deduped\n                            modified = True\n                            if examples_shown < MAX_EXAMPLES:\n                                print(f\"\\nitem_id: {item_id} | Field: '{field}'\")\n                                print(\"  ➤ Before:\", original)\n                                print(\"  ➤ After: \", deduped)\n                                examples_shown += 1\n\n                updated_lines.append(json.dumps(entry))\n\n            except json.JSONDecodeError as e:\n                print(f\"Skipping invalid line in {file_path}: {e}\")\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for line in updated_lines:\n            f.write(line + \"\\n\")\n\nprint(\"\\n✅ Deduplication complete and changes saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:00:06.978201Z","iopub.execute_input":"2025-05-18T08:00:06.978751Z","iopub.status.idle":"2025-05-18T08:00:14.395795Z","shell.execute_reply.started":"2025-05-18T08:00:06.978731Z","shell.execute_reply":"2025-05-18T08:00:14.394975Z"}},"outputs":[{"name":"stdout","text":"\nitem_id: B07CTPR73M | Field: 'item_keywords'\n  ➤ Before: ['love', 'loveseat', 'queen', 'for', 'couch', 'chesterfield', 'rolled', 'couches', 'button', 'homelegance', 'red', 'daybed', 'and', 'trundle', 'savonburg', 'power', 'arm', 'reclining', 'farmhouse', 'a', 'sofa', 'loveseats', 'living', 'set', 'sets', 'room', 'leather', 'upholstered', 'seat', 'with', 'size', 'sofas', 'fabric', 'silver', 'tufted', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill']\n  ➤ After:  ['love', 'loveseat', 'queen', 'for', 'couch', 'chesterfield', 'rolled', 'couches', 'button', 'homelegance', 'red', 'daybed', 'and', 'trundle', 'savonburg', 'power', 'arm', 'reclining', 'farmhouse', 'a', 'sofa', 'loveseats', 'living', 'set', 'sets', 'room', 'leather', 'upholstered', 'seat', 'with', 'size', 'sofas', 'fabric', 'silver', 'tufted', 'vanity', 'outdoor fountain', 'wind spinners', 'windmill']\n\nitem_id: B0853X2F4M | Field: 'item_keywords'\n  ➤ Before: ['cellphonecover', 'backcase', 'mobileguard', 'mobilecover', 'cellphonecase', 'phonecover', 'case', 'protectivecase', 'cover', 'backcover', 'phoneguard', 'mobilecase', 'protectivecover', 'phonecase', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover']\n  ➤ After:  ['cellphonecover', 'backcase', 'mobileguard', 'mobilecover', 'cellphonecase', 'phonecover', 'case', 'protectivecase', 'cover', 'backcover', 'phoneguard', 'mobilecase', 'protectivecover', 'phonecase', 'Back Cover', 'Designer Autumn Girl Mobile Cover', 'Designer Case', 'Hard Case', 'Mi Redmi Go Case', 'Printed Cover', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover']\n\nitem_id: B0853WVZ6Q | Field: 'item_keywords'\n  ➤ Before: ['cellphonecover', 'backcase', 'mobileguard', 'mobilecover', 'cellphonecase', 'phonecover', 'case', 'protectivecase', 'cover', 'backcover', 'phoneguard', 'mobilecase', 'protectivecover', 'phonecase', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover']\n  ➤ After:  ['cellphonecover', 'backcase', 'mobileguard', 'mobilecover', 'cellphonecase', 'phonecover', 'case', 'protectivecase', 'cover', 'backcover', 'phoneguard', 'mobilecase', 'protectivecover', 'phonecase', 'Back Cover', 'Designer Butterflies Neon Light Mobile Cover', 'Designer Case', 'Hard Case', 'Printed Cover', 'Vivo Y81i Case', 'cases and covers', 'fashion case', 'mobile cover', 'polycarbonate cover']\n\n✅ Deduplication complete and changes saved.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"d = 0\nfor file_path in json_files:\n    count = 0\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.strip():  \n                count += 1\n        d+=count\n    file_name = os.path.basename(file_path)\n    print(f\"{file_name}: {count} entries\")\nprint(d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T08:00:14.396596Z","iopub.execute_input":"2025-05-18T08:00:14.397238Z","iopub.status.idle":"2025-05-18T08:00:14.560581Z","shell.execute_reply.started":"2025-05-18T08:00:14.397206Z","shell.execute_reply":"2025-05-18T08:00:14.560040Z"}},"outputs":[{"name":"stdout","text":"listings_0.json: 9232 entries\nlistings_5.json: 9232 entries\nlistings_d.json: 9232 entries\nlistings_2.json: 9232 entries\nlistings_a.json: 9232 entries\nlistings_3.json: 9232 entries\nlistings_6.json: 9232 entries\nlistings_1.json: 9232 entries\nlistings_b.json: 9232 entries\nlistings_f.json: 9222 entries\nlistings_8.json: 9232 entries\nlistings_7.json: 9232 entries\nlistings_c.json: 9232 entries\nlistings_9.json: 9232 entries\nlistings_4.json: 9232 entries\nlistings_e.json: 9232 entries\n147702\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"42 images don't have Json data","metadata":{}}]}